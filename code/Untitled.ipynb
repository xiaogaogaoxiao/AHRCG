{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b789d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pymanopt import tools\n",
    "from pymanopt.solvers.solver import Solver\n",
    "\n",
    "\n",
    "# BetaTypes of the conjugate gradient method in pymanopt was changed.\n",
    "BetaTypes = tools.make_enum(\"BetaTypes\", \"DaiYuan PolakRibiere Hybrid1 Hybrid2\".split())\n",
    "\n",
    "\n",
    "class ConjugateGradient(Solver):\n",
    "    \"\"\"\n",
    "    Module containing conjugate gradient algorithm based on\n",
    "    conjugategradient.m from the manopt MATLAB package.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta_type=BetaTypes.DaiYuan, orth_value=np.inf, linesearch=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiate gradient solver class.\n",
    "        Variable attributes (defaults in brackets):\n",
    "            - beta_type (BetaTypes.HestenesStiefel)\n",
    "                Conjugate gradient beta rule used to construct the new search\n",
    "                direction\n",
    "            - orth_value (numpy.inf)\n",
    "                Parameter for Powell's restart strategy. An infinite\n",
    "                value disables this strategy. See in code formula for\n",
    "                the specific criterion used.\n",
    "            - linesearch (LineSearchWolfe)\n",
    "                The linesearch method to used.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self._beta_type = beta_type\n",
    "        self._orth_value = orth_value\n",
    "\n",
    "        if linesearch is None:\n",
    "            self._linesearch = LineSearchWolfe()\n",
    "        else:\n",
    "            self._linesearch = linesearch\n",
    "        self.linesearch = None\n",
    "\n",
    "    def solve(self, problem, x=None, reuselinesearch=False):\n",
    "        \"\"\"\n",
    "        Perform optimization using nonlinear conjugate gradient method with\n",
    "        linesearch.\n",
    "        This method first computes the gradient of obj w.r.t. arg, and then\n",
    "        optimizes by moving in a direction that is conjugate to all previous\n",
    "        search directions.\n",
    "        Arguments:\n",
    "            - problem\n",
    "                Pymanopt problem setup using the Problem class, this must\n",
    "                have a .manifold attribute specifying the manifold to optimize\n",
    "                over, as well as a cost and enough information to compute\n",
    "                the gradient of that cost.\n",
    "            - x=None\n",
    "                Optional parameter. Starting point on the manifold. If none\n",
    "                then a starting point will be randomly generated.\n",
    "            - reuselinesearch=False\n",
    "                Whether to reuse the previous linesearch object. Allows to\n",
    "                use information from a previous solve run.\n",
    "        Returns:\n",
    "            - x\n",
    "                Local minimum of obj, or if algorithm terminated before\n",
    "                convergence x will be the point at which it terminated.\n",
    "        \"\"\"\n",
    "        man = problem.manifold\n",
    "        verbosity = problem.verbosity\n",
    "        objective = problem.cost\n",
    "        gradient = problem.grad\n",
    "\n",
    "        if not reuselinesearch or self.linesearch is None:\n",
    "            self.linesearch = deepcopy(self._linesearch)\n",
    "        linesearch = self.linesearch\n",
    "\n",
    "        # If no starting point is specified, generate one at random.\n",
    "        if x is None:\n",
    "            x = man.rand()\n",
    "\n",
    "        # Initialize iteration counter and timer\n",
    "        iter = 0\n",
    "        stepsize = np.nan\n",
    "        time0 = time.time()\n",
    "\n",
    "        if verbosity >= 1:\n",
    "            print(\"Optimizing...\")\n",
    "        if verbosity >= 2:\n",
    "            print(\" iter\\t\\t   cost val\\t    grad. norm\")\n",
    "\n",
    "        # Calculate initial cost-related quantities\n",
    "        cost = objective(x)\n",
    "        grad = gradient(x)\n",
    "        gradnorm = man.norm(x, grad)\n",
    "        def _Pgrad(_x):\n",
    "            return problem.precon(_x, gradient(_x))\n",
    "        Pgrad = problem.precon(x, grad)\n",
    "        gradPgrad = man.inner(x, grad, Pgrad)\n",
    "\n",
    "        # Initial descent direction is the negative gradient\n",
    "        desc_dir = -Pgrad\n",
    "\n",
    "        self._start_optlog(extraiterfields=['gradnorm'],\n",
    "                           solverparams={'beta_type': self._beta_type,\n",
    "                                         'orth_value': self._orth_value,\n",
    "                                         'linesearcher': linesearch})\n",
    "\n",
    "        while True:\n",
    "            if verbosity >= 2:\n",
    "                print(\"%5d\\t%+.16e\\t%.8e\" % (iter, cost, gradnorm))\n",
    "\n",
    "            if self._logverbosity >= 2:\n",
    "                self._append_optlog(iter, x, cost, gradnorm=gradnorm)\n",
    "\n",
    "            stop_reason = self._check_stopping_criterion(time0, gradnorm=gradnorm, iter=iter + 1, stepsize=stepsize)\n",
    "\n",
    "            if stop_reason:\n",
    "                if verbosity >= 1:\n",
    "                    print(stop_reason)\n",
    "                    print('')\n",
    "                break\n",
    "\n",
    "            # The line search algorithms require the directional derivative of\n",
    "            # the cost at the current point x along the search direction.\n",
    "            df0 = man.inner(x, grad, desc_dir)\n",
    "\n",
    "            # If we didn't get a descent direction: restart, i.e., switch to\n",
    "            # the negative gradient. Equivalent to resetting the CG direction\n",
    "            # to a steepest descent step, which discards the past information.\n",
    "            if df0 >= 0:\n",
    "                # Or we switch to the negative gradient direction.\n",
    "                if verbosity >= 3:\n",
    "                    print(\"Conjugate gradient info: got an ascent direction \"\n",
    "                          \"(df0 = %.2f), reset to the (preconditioned) \"\n",
    "                          \"steepest descent direction.\" % df0)\n",
    "                # Reset to negative gradient: this discards the CG memory.\n",
    "                desc_dir = -Pgrad\n",
    "                df0 = -gradPgrad\n",
    "\n",
    "            # Execute line search\n",
    "            stepsize, newx = linesearch.search(objective, man, x, desc_dir, cost, df0, _Pgrad)\n",
    "\n",
    "            # Compute the new cost-related quantities for newx\n",
    "            newcost = objective(newx)\n",
    "            newgrad = gradient(newx)\n",
    "            newgradnorm = man.norm(newx, newgrad)\n",
    "            Pnewgrad = problem.precon(newx, newgrad)\n",
    "            newgradPnewgrad = man.inner(newx, newgrad, Pnewgrad)\n",
    "\n",
    "            # Apply the CG scheme to compute the next search direction\n",
    "            oldgrad = man.transp(x, newx, grad)\n",
    "            orth_grads = man.inner(newx, oldgrad, Pnewgrad) / newgradPnewgrad\n",
    "\n",
    "            # Powell's restart strategy (see page 12 of Hager and Zhang's\n",
    "            # survey on conjugate gradient methods, for example)\n",
    "            if abs(orth_grads) >= self._orth_value:\n",
    "                beta = 0\n",
    "                desc_dir = -Pnewgrad\n",
    "            else:\n",
    "                desc_dir = man.transp(x, newx, desc_dir)\n",
    "                if self._beta_type == BetaTypes.DaiYuan:\n",
    "                    diff = newgrad - oldgrad\n",
    "                    beta = newgradPnewgrad / man.inner(newx, diff, desc_dir)\n",
    "                elif self._beta_type == BetaTypes.PolakRibiere:\n",
    "                    diff = newgrad - oldgrad\n",
    "                    ip_diff = man.inner(newx, Pnewgrad, diff)\n",
    "                    beta = ip_diff / gradPgrad\n",
    "                elif self._beta_type == BetaTypes.Hybrid1:\n",
    "                    diff = newgrad - oldgrad\n",
    "                    beta_DY = newgradPnewgrad / man.inner(newx, diff, desc_dir)\n",
    "                    ip_diff = man.inner(newx, Pnewgrad, diff)\n",
    "                    try:\n",
    "                        beta_HS = ip_diff / man.inner(newx, diff, desc_dir)\n",
    "                    except ZeroDivisionError:\n",
    "                        beta_HS = 1\n",
    "                    beta = max(0, min(beta_DY, beta_HS))\n",
    "                elif self._beta_type == BetaTypes.Hybrid2:\n",
    "                    diff = newgrad - oldgrad\n",
    "                    beta_DY = newgradPnewgrad / man.inner(newx, diff, desc_dir)\n",
    "                    ip_diff = man.inner(newx, Pnewgrad, diff)\n",
    "                    try:\n",
    "                        beta_HS = ip_diff / man.inner(newx, diff, desc_dir)\n",
    "                    except ZeroDivisionError:\n",
    "                        beta_HS = 1\n",
    "                    c2 = linesearch.c2\n",
    "                    beta = max(-(1 - c2) / (1 + c2) * beta_DY, min(beta_DY, beta_HS))\n",
    "                else:\n",
    "                    types = \", \".join([\"BetaTypes.%s\" % t for t in BetaTypes._fields])\n",
    "                    raise ValueError(\"Unknown beta_type %s. Should be one of %s.\" % (self._beta_type, types))\n",
    "\n",
    "                desc_dir = -Pnewgrad + beta * desc_dir\n",
    "\n",
    "            # Update the necessary variables for the next iteration.\n",
    "            x = newx\n",
    "            cost = newcost\n",
    "            grad = newgrad\n",
    "            Pgrad = Pnewgrad\n",
    "            gradnorm = newgradnorm\n",
    "            gradPgrad = newgradPnewgrad\n",
    "\n",
    "            iter += 1\n",
    "        \n",
    "        return x, iter + 1, time.time() - time0\n",
    "\n",
    "\n",
    "class LineSearchWolfe:\n",
    "    def __init__(self, c1: float=1e-4, c2: float=0.9):\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Wolfe'\n",
    "\n",
    "    def search(self, objective, man, x, d, f0, df0, gradient):\n",
    "        '''\n",
    "        Returns the step size that satisfies the strong Wolfe condition.\n",
    "        Scipy.optimize.line_search in SciPy v1.4.1 modified to Riemannian manifold.\n",
    "\n",
    "        ----------\n",
    "        References\n",
    "        ----------\n",
    "        [1] SciPy v1.4.1 Reference Guide, https://docs.scipy.org/\n",
    "        '''\n",
    "        fc = [0]\n",
    "        gc = [0]\n",
    "        gval = [None]\n",
    "        gval_alpha = [None]\n",
    "\n",
    "        def phi(alpha):\n",
    "            fc[0] += 1\n",
    "            return objective(man.retr(x, alpha * d))\n",
    "\n",
    "        def derphi(alpha):\n",
    "            newx = man.retr(x, alpha * d)\n",
    "            newd = man.transp(x, newx, d)\n",
    "            gc[0] += 1\n",
    "            gval[0] = gradient(newx)  # store for later use\n",
    "            gval_alpha[0] = alpha\n",
    "            return man.inner(newx, gval[0], newd)\n",
    "\n",
    "        gfk = gradient(x)\n",
    "        derphi0 = man.inner(x, gfk, d)\n",
    "\n",
    "        stepsize = _scalar_search_wolfe(phi, derphi, self.c1, self.c2, maxiter=100)\n",
    "        if stepsize is None:\n",
    "            stepsize = 1e-6\n",
    "        \n",
    "        newx = man.retr(x, stepsize * d)\n",
    "        \n",
    "        return stepsize, newx\n",
    "\n",
    "\n",
    "def _scalar_search_wolfe(phi, derphi, c1=1e-4, c2=0.9, maxiter=100):\n",
    "    phi0 = phi(0.)\n",
    "    derphi0 = derphi(0.)\n",
    "    alpha0 = 0\n",
    "    alpha1 = 1.0\n",
    "    phi_a1 = phi(alpha1)\n",
    "    phi_a0 = phi0\n",
    "    derphi_a0 = derphi0\n",
    "    for i in range(maxiter):\n",
    "        if (phi_a1 > phi0 + c1 * alpha1 * derphi0) or ((phi_a1 >= phi_a0) and (i > 1)):\n",
    "            alpha_star, phi_star, derphi_star = _zoom(alpha0, alpha1, phi_a0, phi_a1, derphi_a0, phi, derphi, phi0, derphi0, c1, c2)\n",
    "            break\n",
    "\n",
    "        derphi_a1 = derphi(alpha1)\n",
    "        if (abs(derphi_a1) <= c2 * abs(derphi0)):\n",
    "            alpha_star = alpha1\n",
    "            phi_star = phi_a1\n",
    "            derphi_star = derphi_a1\n",
    "            break\n",
    "\n",
    "        if (derphi_a1 >= 0):\n",
    "            alpha_star, phi_star, derphi_star = _zoom(alpha1, alpha0, phi_a1, phi_a0, derphi_a1, phi, derphi, phi0, derphi0, c1, c2)\n",
    "            break\n",
    "\n",
    "        alpha2 = 2 * alpha1  # increase by factor of two on each iteration\n",
    "        alpha0 = alpha1\n",
    "        alpha1 = alpha2\n",
    "        phi_a0 = phi_a1\n",
    "        phi_a1 = phi(alpha1)\n",
    "        derphi_a0 = derphi_a1\n",
    "    else:\n",
    "        # stopping test maxiter reached\n",
    "        alpha_star = alpha1\n",
    "        phi_star = phi_a1\n",
    "        derphi_star = None\n",
    "        print('The line search algorithm did not converge')\n",
    "    \n",
    "    return alpha_star\n",
    "\n",
    "\n",
    "def _zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo, phi, derphi, phi0, derphi0, c1, c2):\n",
    "    \"\"\"\n",
    "    Part of the optimization algorithm in `_scalar_search_wolfe`.\n",
    "    \"\"\"\n",
    "    maxiter = 10\n",
    "    i = 0\n",
    "    delta1 = 0.2  # cubic interpolant check\n",
    "    delta2 = 0.1  # quadratic interpolant check\n",
    "    phi_rec = phi0\n",
    "    a_rec = 0\n",
    "    while True:\n",
    "        dalpha = a_hi - a_lo\n",
    "        if dalpha < 0:\n",
    "            a, b = a_hi, a_lo\n",
    "        else:\n",
    "            a, b = a_lo, a_hi\n",
    "        if (i > 0):\n",
    "            cchk = delta1 * dalpha\n",
    "            a_j = _cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi, a_rec, phi_rec)\n",
    "        if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk):\n",
    "            qchk = delta2 * dalpha\n",
    "            a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n",
    "            if (a_j is None) or (a_j > b-qchk) or (a_j < a+qchk):\n",
    "                a_j = a_lo + 0.5*dalpha\n",
    "        phi_aj = phi(a_j)\n",
    "        if (phi_aj > phi0 + c1*a_j*derphi0) or (phi_aj >= phi_lo):\n",
    "            phi_rec = phi_hi\n",
    "            a_rec = a_hi\n",
    "            a_hi = a_j\n",
    "            phi_hi = phi_aj\n",
    "        else:\n",
    "            derphi_aj = derphi(a_j)\n",
    "            if abs(derphi_aj) <= c2 * abs(derphi0):\n",
    "                a_star = a_j\n",
    "                val_star = phi_aj\n",
    "                valprime_star = derphi_aj\n",
    "                break\n",
    "            if derphi_aj*(a_hi - a_lo) >= 0:\n",
    "                phi_rec = phi_hi\n",
    "                a_rec = a_hi\n",
    "                a_hi = a_lo\n",
    "                phi_hi = phi_lo\n",
    "            else:\n",
    "                phi_rec = phi_lo\n",
    "                a_rec = a_lo\n",
    "            a_lo = a_j\n",
    "            phi_lo = phi_aj\n",
    "            derphi_lo = derphi_aj\n",
    "        i += 1\n",
    "        if (i > maxiter):\n",
    "            # Failed to find a conforming step size\n",
    "            a_star = None\n",
    "            val_star = None\n",
    "            valprime_star = None\n",
    "            break\n",
    "    return a_star, val_star, valprime_star\n",
    "\n",
    "\n",
    "def _cubicmin(a, fa, fpa, b, fb, c, fc):\n",
    "    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            C = fpa\n",
    "            db = b - a\n",
    "            dc = c - a\n",
    "            denom = (db * dc) ** 2 * (db - dc)\n",
    "            d1 = np.empty((2, 2))\n",
    "            d1[0, 0] = dc ** 2\n",
    "            d1[0, 1] = -db ** 2\n",
    "            d1[1, 0] = -dc ** 3\n",
    "            d1[1, 1] = db ** 3\n",
    "            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db,\n",
    "                                            fc - fa - C * dc]).flatten())\n",
    "            A /= denom\n",
    "            B /= denom\n",
    "            radical = B * B - 3 * A * C\n",
    "            xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _quadmin(a, fa, fpa, b, fb):\n",
    "    # f(x) = B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            D = fa\n",
    "            C = fpa\n",
    "            db = b - a * 1.0\n",
    "            B = (fb - D - C * db) / (db * db)\n",
    "            xmin = a - C / (2.0 * B)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ceb809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
